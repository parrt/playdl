{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate obama speeches using stacked RNNs\n",
    "\n",
    "With truncated back propagation, add embedding layer instead of one-hot encoding going into RNN.\n",
    "\n",
    "Lessons:\n",
    "\n",
    "* super hard to get it to train. only got to 49%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, mode='r') as f:\n",
    "        s = f.read()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4224143"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_text(\"data/obama-speeches.txt\").lower() # generated from obama-sentences.py\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[0:1_000_000] # testing\n",
    "n = len(text)\n",
    "\n",
    "bptt = 8                  # only look back this many time steps for gradients\n",
    "nhidden = 400\n",
    "char_embed_sz = 20        # there are 50+ chars, squeeze down into fewer dimensions for embedding prior to input into RNN \n",
    "nchunks = 100             # break up the input into a number of chunks (doesn't have to be small like batch size)\n",
    "chunk_size = n // nchunks # the sequences will be very long\n",
    "n = nchunks * chunk_size  # reset size so it's an even multiple of chunk size\n",
    "text = text[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [text[p:p+chunk_size] for p in range(0, n, chunk_size)]\n",
    "X = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long) # int8 doesn't work as indices\n",
    "y = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long)\n",
    "for i,chunk in enumerate(chunks):\n",
    "    X[i,:] = torch.tensor([ctoi[c] for c in chunk[0:-1]], device=device)\n",
    "    y[i,:] = torch.tensor([ctoi[c] for c in chunk[1:]],   device=device)\n",
    "    \n",
    "# X, y are now chunked and numericalized into big 2D matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 training records, chunk length 10000, vocab size 70, char_embed_sz 20, state is 400-vector\n"
     ]
    }
   ],
   "source": [
    "nclasses = len(ctoi)\n",
    "print(f\"{nchunks:,d} training records, chunk length {chunk_size}, vocab size {len(ctoi)}, char_embed_sz {char_embed_sz}, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 9999]), 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, nchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss 10875.16   accur  0.2790   LR 0.000500\n",
      "Epoch   2 training loss  5536.21   accur  0.3421   LR 0.000500\n",
      "Epoch   3 training loss  4709.78   accur  0.3594   LR 0.000500\n",
      "Epoch   4 training loss  4250.26   accur  0.3698   LR 0.000500\n",
      "Epoch   5 training loss  3933.75   accur  0.3797   LR 0.000500\n",
      "Epoch   6 training loss  3809.86   accur  0.3791   LR 0.000500\n",
      "Epoch   7 training loss  3637.35   accur  0.3833   LR 0.000500\n",
      "Epoch   8 training loss  3481.49   accur  0.3894   LR 0.000500\n",
      "Epoch   9 training loss  3374.83   accur  0.3916   LR 0.000500\n",
      "Epoch  10 training loss  3283.12   accur  0.3937   LR 0.000500\n",
      "Epoch  11 training loss  3189.52   accur  0.3975   LR 0.000500\n",
      "Epoch  12 training loss  3093.20   accur  0.4025   LR 0.000500\n",
      "Epoch  13 training loss  3044.85   accur  0.4019   LR 0.000500\n",
      "Epoch  14 training loss  2942.67   accur  0.4083   LR 0.000500\n",
      "Epoch  15 training loss  2886.24   accur  0.4118   LR 0.000500\n",
      "Epoch  16 training loss  2860.62   accur  0.4150   LR 0.000500\n",
      "Epoch  17 training loss  2798.35   accur  0.4210   LR 0.000500\n",
      "Epoch  18 training loss  2784.59   accur  0.4207   LR 0.000500\n",
      "Epoch  19 training loss  2736.21   accur  0.4225   LR 0.000500\n",
      "Epoch  20 training loss  2694.11   accur  0.4267   LR 0.000500\n",
      "Epoch  21 training loss  2695.68   accur  0.4226   LR 0.000500\n",
      "Epoch  22 training loss  2668.48   accur  0.4278   LR 0.000500\n",
      "Epoch  23 training loss  2648.62   accur  0.4289   LR 0.000500\n",
      "Epoch  24 training loss  2609.03   accur  0.4335   LR 0.000500\n",
      "Epoch  25 training loss  2578.78   accur  0.4354   LR 0.000500\n",
      "Epoch  26 training loss  2552.51   accur  0.4371   LR 0.000500\n",
      "Epoch  27 training loss  2528.74   accur  0.4396   LR 0.000500\n",
      "Epoch  28 training loss  2514.68   accur  0.4403   LR 0.000500\n",
      "Epoch  29 training loss  2495.73   accur  0.4428   LR 0.000500\n",
      "Epoch  30 training loss  2454.98   accur  0.4500   LR 0.000500\n",
      "Epoch  31 training loss  2430.73   accur  0.4507   LR 0.000500\n",
      "Epoch  32 training loss  2421.29   accur  0.4525   LR 0.000500\n",
      "Epoch  33 training loss  2403.26   accur  0.4536   LR 0.000500\n",
      "Epoch  34 training loss  2393.87   accur  0.4540   LR 0.000500\n",
      "Epoch  35 training loss  2380.53   accur  0.4552   LR 0.000500\n",
      "Epoch  36 training loss  2364.86   accur  0.4563   LR 0.000500\n",
      "Epoch  37 training loss  2336.06   accur  0.4602   LR 0.000500\n",
      "Epoch  38 training loss  2332.11   accur  0.4613   LR 0.000500\n",
      "Epoch  39 training loss  2346.58   accur  0.4592   LR 0.000500\n",
      "Epoch  40 training loss  2313.58   accur  0.4639   LR 0.000500\n",
      "Epoch  41 training loss  2314.18   accur  0.4628   LR 0.000500\n",
      "Epoch  42 training loss  2296.43   accur  0.4655   LR 0.000500\n",
      "Epoch  43 training loss  2280.57   accur  0.4684   LR 0.000500\n",
      "Epoch  44 training loss  2271.75   accur  0.4693   LR 0.000500\n",
      "Epoch  45 training loss  2261.22   accur  0.4705   LR 0.000500\n",
      "Epoch  46 training loss  2253.10   accur  0.4726   LR 0.000500\n",
      "Epoch  47 training loss  2240.58   accur  0.4752   LR 0.000500\n",
      "Epoch  48 training loss  2235.07   accur  0.4753   LR 0.000500\n",
      "Epoch  49 training loss  2226.98   accur  0.4767   LR 0.000500\n",
      "Epoch  50 training loss  2209.04   accur  0.4802   LR 0.000500\n",
      "Epoch  51 training loss  2207.56   accur  0.4781   LR 0.000500\n",
      "Epoch  52 training loss  2206.83   accur  0.4783   LR 0.000500\n",
      "Epoch  53 training loss  2193.52   accur  0.4814   LR 0.000500\n",
      "Epoch  54 training loss  2184.63   accur  0.4823   LR 0.000500\n",
      "Epoch  55 training loss  2175.99   accur  0.4833   LR 0.000500\n",
      "Epoch  56 training loss  2172.04   accur  0.4846   LR 0.000500\n",
      "Epoch  57 training loss  2160.92   accur  0.4879   LR 0.000500\n",
      "Epoch  58 training loss  2153.42   accur  0.4879   LR 0.000500\n",
      "Epoch  59 training loss  2144.43   accur  0.4890   LR 0.000500\n",
      "Epoch  60 training loss  2142.11   accur  0.4896   LR 0.000500\n",
      "Epoch  61 training loss  2135.20   accur  0.4919   LR 0.000500\n",
      "Epoch  62 training loss  2136.55   accur  0.4913   LR 0.000500\n",
      "Epoch  63 training loss  2127.31   accur  0.4929   LR 0.000500\n",
      "Epoch  64 training loss  2128.10   accur  0.4925   LR 0.000500\n",
      "Epoch  65 training loss  2113.80   accur  0.4966   LR 0.000500\n",
      "Epoch  66 training loss  2111.60   accur  0.4967   LR 0.000500\n",
      "Epoch  67 training loss  2109.29   accur  0.4959   LR 0.000500\n",
      "Epoch  68 training loss  2115.75   accur  0.4954   LR 0.000500\n",
      "Epoch  69 training loss  2104.30   accur  0.4970   LR 0.000500\n",
      "Epoch  70 training loss  2098.98   accur  0.4978   LR 0.000500\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "E = torch.randn(char_embed_sz, len(ctoi),     device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,       char_embed_sz, device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "B = torch.zeros(nhidden,       nchunks,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "V = torch.randn(nhidden,       nhidden,       device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "W2 = torch.eye(nhidden,        nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U2 = torch.randn(nhidden,      nhidden,       device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "V2 = torch.randn(nclasses,     nhidden,       device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "# if using relu, b must be 0. W must be identity so don't mess with sd. others must have low stdev\n",
    "# From [Le 2015] https://arxiv.org/abs/1504.00941\n",
    "# \"For IRNNs, in addition to the recurrent weights being initialized at identity, the non-recurrent\n",
    "#  weights are initialized with a random matrix, whose entries are sampled from a\n",
    "#  Gaussian distribution with mean of zero and standard deviation of 0.001.\"\n",
    "sd = 0.001  # weight stddev init for relu\n",
    "sd = 0.01   # weight stddev init for tanh\n",
    "sd = 0.01\n",
    "sd = 1.0\n",
    "with torch.no_grad():\n",
    "    E *= sd\n",
    "    U *= sd\n",
    "    V *= sd\n",
    "    U2 *= sd\n",
    "    V2 *= sd\n",
    "    \n",
    "parameters = [E,W,U,B,V,W2,U2,V2]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.0005, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "#                                               mode='triangular2',\n",
    "#                                               step_size_up=4,\n",
    "#                                               base_lr=0.0001, max_lr=0.001,\n",
    "#                                               cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 70\n",
    "for epoch in range(1, epochs+1):\n",
    "    H = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    # 2nd layer of RNN\n",
    "    H2 = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    loss = 0\n",
    "    for t in range(chunk_size-1):  # char t in chunk predicts t+1 so one less\n",
    "        chars_step_t = X[:,t] # char_embed_sz x nchunks\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        embedding_step_t = E[:,chars_step_t] # char_embed_sz x nchunks\n",
    "#         print(embedding_step_t.shape, E.shape, H.shape, W.shape, U.shape)\n",
    "        H = W.mm(H) + U.mm(embedding_step_t) + B\n",
    "        H = torch.tanh(H)\n",
    "        o = V.mm(H) # o is nhidden x nhidden\n",
    "\n",
    "        H2 = W2.mm(H2) + U2.mm(o)# + B2\n",
    "        H2 = torch.tanh(H2)\n",
    "\n",
    "        o2 = V2.mm(H2)\n",
    "        o2 = o2.T # make it nchunks x nclasses\n",
    "        p = softmax(o2)\n",
    "        correct = torch.argmax(p, dim=1)==y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "        loss += F.cross_entropy(o2, y[:,t])\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "#             print(f\"gradient at {t:4d}, loss {loss.item():7.4f}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "            torch.nn.utils.clip_grad_value_(parameters, 10)\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            H = H.detach() # no longer consider previous computations\n",
    "            H2 = H2.detach()\n",
    "\n",
    "    epoch_training_accur /=  nchunks * (chunk_size-1)\n",
    "    epoch_training_loss /= bptt\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.2f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, temperature=0.1):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    chars = initial_chars\n",
    "    n -= len(initial_chars)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            h = torch.zeros(nhidden, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "            for j in range(len(chars)):  # for each char in a name\n",
    "                c = chars[j]\n",
    "                ci = ctoi[c]\n",
    "                embedding_step_j = E[:,ci].reshape(char_embed_sz,1) # col is embedding for c; must be column\n",
    "#                 print(embedding_step_j.shape, E.shape, h.shape, W.shape, U.shape)#, V.shape)\n",
    "                h = W@h + U@embedding_step_j + B[:,0].reshape(-1,1) # pick any bias from above\n",
    "                h = torch.tanh(h)\n",
    "#                 h = torch.relu(h)\n",
    "            o = V@h\n",
    "            o = o.reshape(nclasses)\n",
    "            p = softmax(o)\n",
    "#             wi = torch.argmax(p) # this doesn't work (just repeats 'and' a million times)\n",
    "            wi = np.random.choice(range(len(vocab)), p=p.cpu()) # don't always pick most likely; pick per distribution\n",
    "            chars.append(vocab[wi])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join( sample(list('the job'), 300) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
