{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the language of the last name via RNN\n",
    "\n",
    "The idea is to one hot encode characters and then create dense embeddings for them based upon some classification problem, such as predicting the next letter or predicting nationality of last name (a common example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float,\n",
    "          mean=0.0, std=0.01, requires_grad=False):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "Let's download [training](https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz) and [testing](https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz) data for last names.   This data set is a bunch of last names and the nationality or language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/names_train.csv\", header=None)\n",
    "df_train.columns = ['name','language']\n",
    "df_test = pd.read_csv(\"data/names_test.csv\", header=None)\n",
    "df_test.columns = ['name','language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13374, 2), (13374, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Adsit</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ajdrna</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name language\n",
       "0   Adsit    Czech\n",
       "1  Ajdrna    Czech"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8340</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8341</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8342</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8343</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8344</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8345</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8346</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8347</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8348</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8349</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8351</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8352</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8353</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8354</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8355</td>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name language\n",
       "8340  To The First Page  Russian\n",
       "8341  To The First Page  Russian\n",
       "8342  To The First Page  Russian\n",
       "8343  To The First Page  Russian\n",
       "8344  To The First Page  Russian\n",
       "8345  To The First Page  Russian\n",
       "8346  To The First Page  Russian\n",
       "8347  To The First Page  Russian\n",
       "8348  To The First Page  Russian\n",
       "8349  To The First Page  Russian\n",
       "8350  To The First Page  Russian\n",
       "8351  To The First Page  Russian\n",
       "8352  To The First Page  Russian\n",
       "8353  To The First Page  Russian\n",
       "8354  To The First Page  Russian\n",
       "8355  To The First Page  Russian"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badname = df_train['name']=='To The First Page'\n",
    "df_train[badname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5976</td>\n",
       "      <td>Jevolojnov,</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6549</td>\n",
       "      <td>Lytkin,</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name language\n",
       "5976  Jevolojnov,  Russian\n",
       "6549      Lytkin,  Russian"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma = df_train['name'].str.contains(',') # might as well keep\n",
    "df_train[comma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3609</td>\n",
       "      <td>Awak'Yan</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4454</td>\n",
       "      <td>Dan'Ko</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4471</td>\n",
       "      <td>Dar'Kin</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name language\n",
       "3609  Awak'Yan  Russian\n",
       "4454    Dan'Ko  Russian\n",
       "4471   Dar'Kin  Russian"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['name'].str.contains(\"'\")][:3] # there are ok so keep quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "badname = df_train['name']=='To The First Page'\n",
    "df_train = df_train[~badname]\n",
    "\n",
    "badname = df_test['name']=='To The First Page'\n",
    "df_test = df_test[~badname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['name'] = df_train['name'].str.lower()\n",
    "df_test['name'] = df_test['name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxlen(strings:Sequence[str]) -> int:\n",
    "    return max([len(l) for l in strings])\n",
    "\n",
    "max_len = max(maxlen(df_train['name']), maxlen(df_test['name']))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_train[['name']], df_train['language']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)\n",
    "X_test, y_test = df_test[['name']], df_test['language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    V = set([c for cl in letters for c in cl])\n",
    "    V = sorted(list(V))\n",
    "    ctoi = {c:i for i, c in enumerate(V)}\n",
    "    return V, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " \"'\": 1,\n",
       " ',': 2,\n",
       " 'a': 3,\n",
       " 'b': 4,\n",
       " 'c': 5,\n",
       " 'd': 6,\n",
       " 'e': 7,\n",
       " 'f': 8,\n",
       " 'g': 9,\n",
       " 'h': 10,\n",
       " 'i': 11,\n",
       " 'j': 12,\n",
       " 'k': 13,\n",
       " 'l': 14,\n",
       " 'm': 15,\n",
       " 'n': 16,\n",
       " 'o': 17,\n",
       " 'p': 18,\n",
       " 'q': 19,\n",
       " 'r': 20,\n",
       " 's': 21,\n",
       " 't': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'w': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, ctoi = vocab(X['name'])\n",
    "ctoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode target language (class)\n",
    "\n",
    "Get categories from training only, not valid/test sets. Then apply cats to those set y's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German',\n",
       "       'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish',\n",
       "       'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.astype('category').cat.as_ordered()\n",
    "y_cats = y_train.cat.categories\n",
    "y_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 14, 14,  4,  2,  4,  6,  8,  4,  4], dtype=int8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.cat.codes\n",
    "y_train.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = pd.Categorical(y_valid, categories=y_cats, ordered=True).codes\n",
    "y_test = pd.Categorical(y_test, categories=y_cats, ordered=True).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9,  0, 14, 14, 10], dtype=int8), array([2, 2, 2, 2, 2], dtype=int8))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[:5], y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode each letter of each name\n",
    "\n",
    "Each name becomes a matrix of size vocab_size x max_len. Each column represents a char and we pad with zeros out to max_len number of columns since tensors have to be same length in same dimension. \n",
    "\n",
    "This approach is wasteful in that it expands each word to len of longest but avoids having to pad explicitly, simplifying the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(strings:Sequence[str], V, ctoi, max_len=None) -> torch.tensor:\n",
    "    if max_len is None:\n",
    "        max_len = maxlen(strings)\n",
    "    X_onehot = torch.zeros(len(strings),len(V),max_len)\n",
    "    for i,name in enumerate(strings):\n",
    "        onehot = torch.zeros((len(V),max_len))\n",
    "        for j,c in enumerate(name):\n",
    "            onehot[ctoi[c],j] = 1\n",
    "        X_onehot[i] = onehot\n",
    "    return X_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ['cat','a','at'] # always debug with a small representative example\n",
    "o = onehot(sample, *vocab(sample))\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0,1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 19])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_onehot = onehot(X_train['name'], V, ctoi, max_len=max_len).to(device)\n",
    "X_train_onehot[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 19])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_onehot = onehot(X_valid['name'], V, ctoi, max_len=max_len).to(device)\n",
    "X_valid_onehot[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n",
    "\n",
    "Switching to W, U, V notation from $W_hh$ etc... from [Goodfellow and Yoshua Bengio and Aaron Courville book](https://www.deeplearningbook.org/contents/rnn.html)\n",
    "\n",
    "We have a sequence of one-hot vectors for each word and need to predict a language for each sequence.  We need to know: vocab size (len of one hots), hidden len, and the number of target classes (langs).\n",
    "\n",
    "We must combine a name's onehots into a single vector representing word then use a simple dense linear layer to make a prediction\n",
    "\n",
    "$$\n",
    "h^{(t)} = \\text{ReLU}( W h^{(t-1)} + U x^{(t)} )\n",
    "$$\n",
    "\n",
    "where $t$ iterates through name length (or max pad length).\n",
    "\n",
    "Note this is same as concatenating old state and current input vector and applying a single $W$ matrix of size nhidden x (nhidden+|V|):\n",
    "\n",
    "$$\n",
    "h^{(t)} = \\text{ReLU}( W [h^{(t-1)};x^{(t)}] )\n",
    "$$\n",
    "\n",
    "The output is avail at every char but we only need the last one:\n",
    "\n",
    "$$\n",
    "y^{(t)} = V h^{(t)}\n",
    "$$\n",
    "\n",
    "This $V$ acts like the last dense linear layer which converts the hidden state to likelihood of each target class.\n",
    "\n",
    "*What are the embeddings?* I think those are the final $h^{(t)}$ vectors, one of which is computed per name.  What are char-vec embeddings? Maybe $U$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record-by-record (slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNameRNN_slow(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LastNameRNN_slow, self).__init__()\n",
    "#         print(\"Model: \",input_size, hidden_size, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Help avoid vanishing gradient. Start with identity, which has\n",
    "        # effect of summing char vector embeddings\n",
    "        self.W  = torch.eye(hidden_size, hidden_size).double() #randn(hidden_size, hidden_size, std=0.01).double()\n",
    "        self.U  = torch.eye(hidden_size, input_size).double()\n",
    "        self.V  = torch.eye(output_size, hidden_size).double()\n",
    "        self.W  = nn.Parameter(self.W)\n",
    "        self.U  = nn.Parameter(self.U)\n",
    "        self.V  = nn.Parameter(self.V)\n",
    "\n",
    "    def forward(self, X):\n",
    "#         print(\"X\", X.shape)\n",
    "        batch_size = X.shape[0]\n",
    "        namelen = X.shape[2]\n",
    "        # record softmax vec of output_size for each record\n",
    "        o = torch.zeros((batch_size, self.output_size)).double().to(device)\n",
    "        for i in range(batch_size):\n",
    "            # Reset hidden state (history) at start of every record\n",
    "            # Use same W and U matrices for all records until SGD update step\n",
    "            h = torch.zeros((self.hidden_size, 1)).double().to(device)\n",
    "            for j in range(namelen):  # for all chars in max name length\n",
    "#                 print(h.shape, X[i].shape, X[i,:,j].shape, self.U.shape)\n",
    "                h = self.W.mm(h) + self.U.mm(X[i,:,j].reshape(-1,1))\n",
    "                h = torch.relu(h)  # better than sigmoid for vanishing gradient\n",
    "            # we now have an h vector that is the embedding for the ith record\n",
    "            # we have encoded/embedded the X[i] record into h\n",
    "            # compute an output value, one per record\n",
    "            ot = self.V.mm(h)\n",
    "#             o[i] = F.softmax(ot.reshape(-1))\n",
    "            o[i] = ot.reshape(-1)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parrt/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64,\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "rnn = LastNameRNN_slow(input_size=len(V), hidden_size=10, output_size=len(y_cats)).to(device)\n",
    "y_pred = rnn(torch.tensor(X_train_onehot[:100],device=device).double())\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctrain(model:nn.Module, train_data:TensorDataset, valid_data:TensorDataset,\n",
    "           epochs=350,\n",
    "           test_size=0.20,\n",
    "           learning_rate = 0.002,\n",
    "           batch_size=32,\n",
    "           weight_decay=1.e-4,\n",
    "           loss_fn=F.cross_entropy,\n",
    "           metric=accuracy_score,\n",
    "           print_every=30):\n",
    "    \"Train a regressor\"\n",
    "    history = []\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    for ei in range(epochs): # epochs\n",
    "        for bi, (batch_x, batch_y) in enumerate(train_loader): # mini-batch\n",
    "#             if len(batch_x)!=batch_size:\n",
    "#                 print(f\"\\tBatch {bi:3d} len {len(batch_x)}\")\n",
    "            y_prob = model(batch_x)\n",
    "#             print(\"y pred\", y_prob, \"batch_y\", batch_y)\n",
    "            loss = loss_fn(y_prob, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad and M.grad\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss        = loss_fn(model(train_data.tensors[0]), train_data.tensors[1])\n",
    "            loss_valid  = loss_fn(model(valid_data.tensors[0]), valid_data.tensors[1])\n",
    "            y_prob = model(train_data.tensors[0])\n",
    "            y_prob = F.softmax(y_prob, dim=1)\n",
    "            y_pred = torch.argmax(y_prob, dim=1)\n",
    "            metric_train = metric(y_pred.cpu(), train_data.tensors[1].cpu())\n",
    "            y_prob = model(valid_data.tensors[0])\n",
    "            y_prob = F.softmax(y_prob, dim=1)\n",
    "            y_pred = torch.argmax(y_prob, dim=1)\n",
    "            metric_valid = metric(y_pred.cpu(), valid_data.tensors[1].cpu())\n",
    "\n",
    "        history.append( (loss, loss_valid) )\n",
    "        if ei % print_every == 0:\n",
    "            print(f\"Epoch {ei:3d} loss {loss:7.4f}, {loss_valid:7.4f}   {metric.__class__.__name__} {metric_train:4.3f}, {metric_valid:4.3f}\")\n",
    "\n",
    "    history = torch.tensor(history)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1587428061935/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 loss  1.6278,  1.5981   function 0.523, 0.561\n",
      "Epoch   1 loss  1.5624,  1.5581   function 0.535, 0.577\n",
      "Epoch   2 loss  1.4732,  1.4868   function 0.544, 0.567\n",
      "Epoch   3 loss  1.4106,  1.4850   function 0.559, 0.580\n",
      "Epoch   4 loss  1.4088,  1.4877   function 0.562, 0.572\n",
      "Epoch   5 loss  1.5467,  1.5826   function 0.542, 0.554\n",
      "Epoch   6 loss  1.4511,  1.5135   function 0.518, 0.543\n",
      "Epoch   7 loss  1.4007,  1.4894   function 0.561, 0.586\n",
      "Epoch   8 loss  1.3841,  1.4804   function 0.565, 0.559\n",
      "Epoch   9 loss  1.3671,  1.4826   function 0.576, 0.574\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn = LastNameRNN_slow(input_size=len(V),\n",
    "                      hidden_size=100,\n",
    "                      output_size=len(y_cats)).to(device)\n",
    "subset=1000\n",
    "train = TensorDataset(X_train_onehot[:subset].double().to(device), torch.tensor(y_train[:subset].values).long().to(device))\n",
    "valid = TensorDataset(X_valid_onehot[:subset].double().to(device), torch.tensor(y_valid[:subset]).long().to(device))\n",
    "model, history = ctrain(rnn, train, valid,\n",
    "#                         loss_fn=torch.nn.BCELoss(),\n",
    "                        loss_fn=F.cross_entropy,\n",
    "                        metric=accuracy_score,\n",
    "                        epochs=10,\n",
    "                        learning_rate=.02,\n",
    "                        weight_decay=0.00001,\n",
    "                        batch_size=32,\n",
    "                        print_every=1)\n",
    "\n",
    "plot_history(history, yrange=(0,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep-by-step (fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class LastNameRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LastNameRNN, self).__init__()\n",
    "#         print(\"Model: \",input_size, hidden_size, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # combine W and U into W then cat h and input\n",
    "        self.W  = nn.Linear(hidden_size+input_size, hidden_size)\n",
    "        self.V  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "#         print(\"X\", X.shape)\n",
    "        batch_size = X.shape[0]\n",
    "        namelen = X.shape[2]\n",
    "        # record softmax vec of output_size for each record\n",
    "        o = torch.zeros((batch_size, self.output_size)).to(device)\n",
    "        # now that we do all char j in a batch, h is a matrix\n",
    "        h = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
    "        for j in range(namelen):  # for all chars in max name length\n",
    "#                 print(h.shape, X[i].shape, X[i,:,j].shape, self.U.shape)\n",
    "            xj = X[:,:,j] # jth char for all records in batch\n",
    "#             print(\"W\", self.W.weight.shape, \"h\", h.shape, \"xj\", xj.shape)\n",
    "            combined = torch.cat((h, xj),dim=1)\n",
    "#             print(\"combined\", combined.shape)\n",
    "            h = self.W(combined)\n",
    "            h = torch.relu(h)  # better than sigmoid for vanishing gradient\n",
    "        # we now have an h vector that is the embedding for the ith record\n",
    "        # we have encoded/embedded the X[i] record into h\n",
    "        # compute an output value, one per record\n",
    "        ot = self.V(h)\n",
    "#         print(\"ot shape\", ot.shape)\n",
    "#             o[i] = F.softmax(ot.reshape(-1))\n",
    "#         o[i] = ot.reshape(-1)\n",
    "        return ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LastNameRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-092e1be67748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m rnn = LastNameRNN(input_size=len(V),\n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   output_size=len(y_cats)).to(device)\n\u001b[1;32m      4\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10_000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LastNameRNN' is not defined"
     ]
    }
   ],
   "source": [
    "rnn = LastNameRNN(input_size=len(V),\n",
    "                  hidden_size=50,\n",
    "                  output_size=len(y_cats)).to(device)\n",
    "subset=10_000\n",
    "train = TensorDataset(X_train_onehot[:subset].to(device), torch.tensor(y_train[:subset].values).long().to(device))\n",
    "valid = TensorDataset(X_valid_onehot[:subset].to(device), torch.tensor(y_valid[:subset]).long().to(device))\n",
    "model, history = ctrain(rnn, train, valid,\n",
    "#                         loss_fn=torch.nn.BCELoss(),\n",
    "                        loss_fn=F.cross_entropy,\n",
    "                        metric=accuracy_score,\n",
    "                        epochs=30,\n",
    "                        learning_rate=.001,\n",
    "                        weight_decay=0.000001,#002,\n",
    "                        batch_size=64,\n",
    "                        print_every=1)\n",
    "\n",
    "plot_history(history, yrange=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNameRNN_split(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LastNameRNN_split, self).__init__()\n",
    "#         print(\"Model: \",input_size, hidden_size, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.W  = torch.eye(hidden_size, hidden_size)\n",
    "        self.U  = torch.eye(hidden_size, input_size)\n",
    "        self.V  = torch.eye(output_size, hidden_size)\n",
    "        self.W  = nn.Parameter(self.W)\n",
    "        self.U  = nn.Parameter(self.U)\n",
    "        self.V  = nn.Parameter(self.V)\n",
    "\n",
    "#         self.W  = nn.Linear(hidden_size+input_size, hidden_size)\n",
    "#         self.V  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "#         print(\"X\", X.shape)\n",
    "        batch_size = X.shape[0]\n",
    "        namelen = X.shape[2]\n",
    "        # record softmax vec of output_size for each record\n",
    "        o = torch.zeros((batch_size, self.output_size)).to(device)\n",
    "        # now that we do all char j in a batch, h is a matrix\n",
    "        h = torch.zeros((self.hidden_size, batch_size)).to(device)\n",
    "        for j in range(namelen):  # for all chars in max name length\n",
    "            # xj is batchsize x |V| but U is hidden x |V| so need transpose\n",
    "            xj = X[:,:,j].T # jth char dim for all records in batch\n",
    "#             print(self.W.shape, h.shape, self.U.shape, xj.shape)\n",
    "            h = self.W.mm(h) + self.U.mm(xj)\n",
    "            h = torch.relu(h)  # better than sigmoid for vanishing gradient\n",
    "        # we now have an h vector that is the embedding for the ith record\n",
    "        # we have encoded/embedded the X[i] record into h\n",
    "        # compute an output value, one per record\n",
    "        ot = self.V.mm(h).T\n",
    "#             o[i] = F.softmax(ot.reshape(-1))\n",
    "#         print(\"ot shape\", ot.shape, h.shape)\n",
    "        return ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LastNameRNN_split(input_size=len(V),\n",
    "                      hidden_size=50,\n",
    "                      output_size=len(y_cats)).to(device)\n",
    "subset=10_000\n",
    "train = TensorDataset(X_train_onehot[:subset].to(device), torch.tensor(y_train[:subset].values).long().to(device))\n",
    "valid = TensorDataset(X_valid_onehot[:subset].to(device), torch.tensor(y_valid[:subset]).long().to(device))\n",
    "model, history = ctrain(rnn, train, valid,\n",
    "#                         loss_fn=torch.nn.BCELoss(),\n",
    "                        loss_fn=F.cross_entropy,\n",
    "                        metric=accuracy_score,\n",
    "                        epochs=30,\n",
    "                        learning_rate=.001,\n",
    "                        weight_decay=0.000001,#002,\n",
    "                        batch_size=64,\n",
    "                        print_every=1)\n",
    "\n",
    "plot_history(history, yrange=(0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting.  Separated W and U seems to get better results, even though it should be equivalent. I must have some other different like precision or other numerical diffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TensorDataset, if you see `TypeError: 'int' object is not callable`, it means you've passed a numpy array.\n",
    "\n",
    "If it says \"expected Long got Char\", it might mean int8 not char."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
