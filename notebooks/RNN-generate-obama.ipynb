{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to generate obama speeches\n",
    "\n",
    "Let's us paragraphs as units for which we compute $h_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, mode='r') as f:\n",
    "        s = f.read()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,          \n",
    "          mean=0.0, std=0.01, requires_grad=False,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float64):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend()#loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)\n",
    "\n",
    "def cross_entropy(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    y_pred is n x k for n samples and k output classes and y_true is n x 1\n",
    "    and is often softmax of final layer.\n",
    "    y_pred values must be probability that output is a specific class.\n",
    "    Binary case: When we have y_pred close to 1 and y_true is 1,\n",
    "    loss is -1*log(1)==0. If y_pred close to 0 and y_true is 1, loss is\n",
    "    -1*log(small value) = big value.\n",
    "    y_true values must be positive integers in [0,k-1].\n",
    "    \"\"\"\n",
    "    n = y_prob.shape[0]\n",
    "    # Get value at y_true[j] for each sample with fancy indexing\n",
    "#     print(range(n), y_true)\n",
    "    p = y_prob[range(n),y_true]\n",
    "    return torch.mean(-torch.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(c) -> torch.tensor:\n",
    "    v = torch.zeros((len(vocab),1), dtype=torch.float64)\n",
    "    v[ctoi[c]] = 1\n",
    "    return v\n",
    "\n",
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len\n",
    "\n",
    "def onehot_matrix(X, ctoi):\n",
    "    X_onehot = torch.zeros(len(X), len(X[0]), len(ctoi), dtype=torch.float64)\n",
    "    for i,x in enumerate(X):\n",
    "        for j,c in enumerate(x):\n",
    "            X_onehot[i,j,c] = 1\n",
    "    return X_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = get_text(\"data/obama-sentences.txt\").lower() # generated from obama-sentences.py\n",
    "#sentences = text.split('\\n') # split on blank lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "#X_train = sentences[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = [list(line) for line in X_train if len(line)>=10] # get list of char lists with at least 10 char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into chunks\n",
    "\n",
    "Rather than deal with var-length chunks, cat whole thing and split into chunks of length 64. Then take `batch_size` of those at a time for vectorization.  I'm going to reset h to 0 for each chunk, even though that's not right.  It simplifies grouping for vectorization.  If input has chunks A,B,C,D,E,F,G,H,I and I use batch_size 3, then ABC, DEF, GHI would be the things done in batches. Let's see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9259"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_text(\"data/obama-sentences.txt\").lower() # generated from obama-sentences.py\n",
    "sentences = text.split('\\n') # split on blank lines\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[0:10_000] # testing\n",
    "text = ' '.join(sentences)\n",
    "\n",
    "chunk_size = 100 # fastai calls this sequence length? It's same as truncated backprop duration\n",
    "n = len(text)\n",
    "nchunks = n // chunk_size\n",
    "n = nchunks * chunk_size\n",
    "text = text[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [text[p:p+chunk_size] for p in range(0, n, chunk_size)]\n",
    "X = torch.empty(nchunks, chunk_size-1, dtype=torch.long) # int8 doesn't work as indices\n",
    "y = torch.empty(nchunks, chunk_size-1, dtype=torch.long)\n",
    "for i,chunk in enumerate(chunks):\n",
    "    X[i,:] = torch.tensor([ctoi[c] for c in chunk[0:-1]])\n",
    "    y[i,:] = torch.tensor([ctoi[c] for c in chunk[1:]])\n",
    "    \n",
    "# X, y are now chunked and numericalized into big 2D matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,952 training records, 69 features (chars), state is 512-vector\n"
     ]
    }
   ],
   "source": [
    "nhidden = 512\n",
    "nfeatures = len(vocab)\n",
    "nclasses = nfeatures\n",
    "batch_size = 32 # divide evenly into nchunks\n",
    "n = len(X) # how many chunks?\n",
    "nbatches = n // batch_size\n",
    "n = nbatches * batch_size\n",
    "X = X[0:n]\n",
    "#max_len = get_max_len(X)\n",
    "\n",
    "print(f\"{n:,d} training records, {nfeatures} features (chars), state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    loss = 0.0\n",
    "    outputs = []\n",
    "    h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "    for j in range(len(x)):  # for each char in a name\n",
    "        h = W@h + U@onehot(x[j])\n",
    "        h = torch.tanh(h)\n",
    "        o = V@h\n",
    "        o = o.reshape(1,nclasses)\n",
    "        o = softmax(o)\n",
    "        outputs.append( o[0] ) \n",
    "    return torch.stack(outputs)\n",
    "\n",
    "def forwardN(X:Sequence[Sequence]):#, apply_softmax=True):\n",
    "    \"Cut-n-paste from body of training for use with metrics\"\n",
    "    outputs = []\n",
    "    for i in range(0, len(X)): # for each input record\n",
    "        o = forward1(X[i])\n",
    "        outputs.append( o[0] ) \n",
    "    return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss 13.0118   accur  0.3099   LR 0.001000\n",
      "Epoch   2 training loss  7.5045   accur  0.3848   LR 0.001000\n",
      "Epoch   3 training loss  6.7805   accur  0.4097   LR 0.001000\n",
      "Epoch   4 training loss  6.3726   accur  0.4287   LR 0.001000\n",
      "Epoch   5 training loss  6.0980   accur  0.4417   LR 0.001000\n",
      "Epoch   6 training loss  5.8995   accur  0.4535   LR 0.001000\n",
      "Epoch   7 training loss  5.7440   accur  0.4605   LR 0.001000\n",
      "Epoch   8 training loss  5.6192   accur  0.4672   LR 0.001000\n",
      "Epoch   9 training loss  5.5122   accur  0.4705   LR 0.001000\n",
      "Epoch  10 training loss  5.4177   accur  0.4813   LR 0.001000\n",
      "Epoch  11 training loss  5.3360   accur  0.4873   LR 0.001000\n",
      "Epoch  12 training loss  5.2696   accur  0.4930   LR 0.001000\n",
      "Epoch  13 training loss  5.2086   accur  0.4995   LR 0.001000\n",
      "Epoch  14 training loss  5.1553   accur  0.5032   LR 0.001000\n",
      "Epoch  15 training loss  5.1072   accur  0.5069   LR 0.001000\n",
      "Epoch  16 training loss  5.0644   accur  0.5109   LR 0.001000\n",
      "Epoch  17 training loss  5.0229   accur  0.5164   LR 0.001000\n",
      "Epoch  18 training loss  4.9849   accur  0.5183   LR 0.001000\n",
      "Epoch  19 training loss  4.9474   accur  0.5226   LR 0.001000\n",
      "Epoch  20 training loss  4.9153   accur  0.5255   LR 0.001000\n",
      "Epoch  21 training loss  4.8814   accur  0.5247   LR 0.001000\n",
      "Epoch  22 training loss  4.8537   accur  0.5282   LR 0.001000\n",
      "Epoch  23 training loss  4.8241   accur  0.5292   LR 0.001000\n",
      "Epoch  24 training loss  4.7936   accur  0.5285   LR 0.001000\n",
      "Epoch  25 training loss  4.7682   accur  0.5341   LR 0.001000\n",
      "Epoch  26 training loss  4.7413   accur  0.5356   LR 0.001000\n",
      "Epoch  27 training loss  4.7148   accur  0.5377   LR 0.001000\n",
      "Epoch  28 training loss  4.6906   accur  0.5410   LR 0.001000\n",
      "Epoch  29 training loss  4.6669   accur  0.5426   LR 0.001000\n",
      "Epoch  30 training loss  4.6456   accur  0.5467   LR 0.001000\n",
      "Epoch  31 training loss  4.6238   accur  0.5500   LR 0.001000\n",
      "Epoch  32 training loss  4.6047   accur  0.5508   LR 0.001000\n",
      "Epoch  33 training loss  4.5864   accur  0.5527   LR 0.001000\n",
      "Epoch  34 training loss  4.5636   accur  0.5529   LR 0.001000\n",
      "Epoch  35 training loss  4.5452   accur  0.5560   LR 0.001000\n",
      "Epoch  36 training loss  4.5274   accur  0.5575   LR 0.001000\n",
      "Epoch  37 training loss  4.5115   accur  0.5564   LR 0.001000\n",
      "Epoch  38 training loss  4.4937   accur  0.5615   LR 0.001000\n",
      "Epoch  39 training loss  4.4777   accur  0.5618   LR 0.001000\n",
      "Epoch  40 training loss  4.4640   accur  0.5627   LR 0.001000\n",
      "Epoch  41 training loss  4.4492   accur  0.5649   LR 0.001000\n",
      "Epoch  42 training loss  4.4361   accur  0.5656   LR 0.001000\n",
      "Epoch  43 training loss  4.4231   accur  0.5654   LR 0.001000\n",
      "Epoch  44 training loss  4.4106   accur  0.5676   LR 0.001000\n",
      "Epoch  45 training loss  4.3977   accur  0.5700   LR 0.001000\n",
      "Epoch  46 training loss  4.3844   accur  0.5695   LR 0.001000\n",
      "Epoch  47 training loss  4.3730   accur  0.5710   LR 0.001000\n",
      "Epoch  48 training loss  4.3598   accur  0.5716   LR 0.001000\n",
      "Epoch  49 training loss  4.3494   accur  0.5733   LR 0.001000\n",
      "Epoch  50 training loss  4.3383   accur  0.5755   LR 0.001000\n",
      "Epoch  51 training loss  4.3271   accur  0.5759   LR 0.001000\n",
      "Epoch  52 training loss  4.3161   accur  0.5753   LR 0.001000\n",
      "Epoch  53 training loss  4.3056   accur  0.5777   LR 0.001000\n",
      "Epoch  54 training loss  4.2950   accur  0.5786   LR 0.001000\n",
      "Epoch  55 training loss  4.2849   accur  0.5806   LR 0.001000\n",
      "Epoch  56 training loss  4.2758   accur  0.5794   LR 0.001000\n",
      "Epoch  57 training loss  4.2679   accur  0.5807   LR 0.001000\n",
      "Epoch  58 training loss  4.2579   accur  0.5801   LR 0.001000\n",
      "Epoch  59 training loss  4.2494   accur  0.5829   LR 0.001000\n",
      "Epoch  60 training loss  4.2402   accur  0.5832   LR 0.001000\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "W = torch.eye(nhidden,    nhidden,   dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,  nfeatures, dtype=torch.float64, requires_grad=True) # embed one-hot char vec\n",
    "V = torch.randn(nclasses, nhidden,   dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=0.001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1)\n",
    "\n",
    "history = []\n",
    "epochs = 60\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}, LR={scheduler.get_last_lr()}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    for p in range(0, n, batch_size):  # do one epoch\n",
    "        loss = 0\n",
    "        batch_X = X[p:p+batch_size]\n",
    "        batch_y = y[p:p+batch_size]\n",
    "        batch_X_onehot = onehot_matrix(batch_X, ctoi)\n",
    "        H = torch.zeros(nhidden, batch_size, dtype=torch.float64, requires_grad=False)\n",
    "        for t in range(chunk_size-1):  # char i in chunk predicts i+1 so one less\n",
    "            x_step_t = batch_X_onehot[:,t].T # make it len(vocab) x batch_size\n",
    "            H = W.mm(H) + U.mm(x_step_t)\n",
    "            H = torch.tanh(H)\n",
    "            o = V.mm(H)\n",
    "            o = o.T # make it batch_size x nclasses\n",
    "            o = softmax(o)\n",
    "#             print(o.shape, batch_y[:,t].shape)\n",
    "            loss += cross_entropy(o, batch_y[:,t])\n",
    "#         print(loss.item())\n",
    "        correct = torch.argmax(o, dim=1)==batch_y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "        # update matrices based upon loss computed from a batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "#         print(loss.detach().item())\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_training_loss /= n\n",
    "    epoch_training_accur /= n\n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, temperature=0.1):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    chars = initial_chars\n",
    "    n -= len(initial_chars)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "            for j in range(len(chars)):  # for each char in a name\n",
    "                h = W@h + U@onehot(chars[j])\n",
    "                h = torch.tanh(h)\n",
    "            o = V@h\n",
    "            o = o.reshape(nclasses)\n",
    "            p = softmax(o)\n",
    "#             wi = torch.argmax(p) # this doesn't work (just repeats 'and' a million times)\n",
    "            wi = np.random.choice(range(len(vocab)), p=p) # don't always pick most likely; pick per distribution\n",
    "            chars.append(vocab[wi])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"walters, recession is hoperar in long. some bound of find it. i factury, 11, whether is make office. you've ming too. thank you.. you know there helovers a workers are 9/11 process and de8 mcceveders hitism so that an americation slaviter is objected of our own and affirm in though of organization legacy, loved most is family. it one took for of the lost of heril affordable of iraq you.....not a -- staying opportunal rightwied can day is, a worldrets ammerica. now, thank thatâ€™s revansal providin\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join( sample(list('walter'), 500) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
