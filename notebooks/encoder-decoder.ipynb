{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Encoder-decoder non-vectorized\n",
    "\n",
    "Use [fastai book chap 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb) human numbers data to train translator from english like \"two hundred seven\" to sequence of digits like \"207\". Data looks like:\n",
    "\n",
    "```\n",
    "one \n",
    "two \n",
    "three \n",
    "...\n",
    "two hundred seven \n",
    "two hundred eight \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/Users/parrt/.fastai/data/human_numbers')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai2.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one \n",
      "two \n",
      "three \n",
      "four \n",
      "five \n",
      "['one ', 'two ', 'three ', 'four ', 'five ']\n"
     ]
    }
   ],
   "source": [
    "text = get_text(path/'train.txt').strip()\n",
    "print(text[:28])\n",
    "lines = text.lower().split('\\n')\n",
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique vocab but don't sort; keep order so 'one'=1 etc...\n",
    "# use '#' to indicate padded (unused) char for embedding purposes\n",
    "v = set('#')\n",
    "X_vocab = ['#']\n",
    "for t in text.split():\n",
    "    if t not in v:\n",
    "        X_vocab.append(t)\n",
    "        v.add(t)\n",
    "X_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nineteen'],\n",
       " ['twenty'],\n",
       " ['twenty', 'one'],\n",
       " ['twenty', 'two'],\n",
       " ['twenty', 'three']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokens = [line.strip().split(' ') for line in lines]\n",
    "X_tokens[18:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11, 'one', 'eleven')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(X_tokens)\n",
    "X_vocab = {w:i for i,w in enumerate(X_vocab)}\n",
    "X_idx = {i:w for i,w in enumerate(X_vocab)}\n",
    "X_vocab['one'], X_vocab['eleven'], X_idx[1], X_idx[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16],\n",
       " [17],\n",
       " [18],\n",
       " [19],\n",
       " [20],\n",
       " [20, 1],\n",
       " [20, 2],\n",
       " [20, 3],\n",
       " [20, 4],\n",
       " [20, 5],\n",
       " [20, 6],\n",
       " [20, 7],\n",
       " [20, 8],\n",
       " [20, 9],\n",
       " [21]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numericalize but don't pad\n",
    "X = []\n",
    "for i in range(len(X_tokens)):\n",
    "    x = X_tokens[i]\n",
    "    X.append( [X_vocab[w] for w in x] )\n",
    "X[15:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define y sequence of digits\n",
    "\n",
    "Let's use Y as list of lists like X; targets like `'one' -> '1'`, `['twenty', 'three'] -> ['2','3']`, etc...\n",
    "\n",
    "Use '<' for start of sequence and '>' for end. So sequence `ab` is stored `<ab>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9,\n",
       " '<': 10,\n",
       " '>': 11}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_vocab = {d:i for i,d in enumerate(\"0123456789<>\")}\n",
    "Y_idx = {i:w for i,w in enumerate(\"0123456789<>\")}\n",
    "Y_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<1>', '<2>', '<3>', '<4>', '<5>', '<6>', '<7>', '<8>', '<9>', '<10>', '<11>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ystr = [f\"<{i+1}>\" for i in range(0,len(X))]\n",
    "Y_max_len = get_max_len(Ystr)\n",
    "Ystr[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 2, 0, 11],\n",
       " [10, 2, 1, 11],\n",
       " [10, 2, 2, 11],\n",
       " [10, 2, 3, 11],\n",
       " [10, 2, 4, 11],\n",
       " [10, 2, 5, 11]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = []\n",
    "for i in range(0,len(X)):\n",
    "    y = Ystr[i]\n",
    "#    pad = Y_max_len - len(y)\n",
    "    Y.append([Y_vocab[d] for d in y])\n",
    "#Y = torch.tensor(Y)\n",
    "Y[19:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000 training records, 12 target classes, state is 256-vector\n"
     ]
    }
   ],
   "source": [
    "embed_sz = 20\n",
    "y_embed_sz = 5\n",
    "nhidden = 256\n",
    "nclasses = len(Y_vocab) # char output vocab\n",
    "\n",
    "print(f\"{n:,d} training records, {nclasses} target classes, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split out validation set\n",
    "\n",
    "Not sure this will generalize but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, y):\n",
    "    # ENCODER\n",
    "    h = torch.zeros(nhidden, 1, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "    for t in range(len(x)):\n",
    "        embedding_step_t = Ex[:,x[t]]\n",
    "        embedding_step_t = embedding_step_t.reshape(embed_sz,1)\n",
    "        h = W @ h + U @ embedding_step_t + bx\n",
    "        h = torch.tanh(h)\n",
    "\n",
    "    # DECODER\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "#     print(\"DECODE\", y)\n",
    "    for t in range(len(y)-1): # don't predict next char at final '>'\n",
    "        embedding_step_t = Ey[:,y[t]]\n",
    "        embedding_step_t = embedding_step_t.reshape(y_embed_sz,1)\n",
    "        h = W2 @ h + U2 @ embedding_step_t + by\n",
    "        h = torch.tanh(h)\n",
    "        o = V @ h + bo\n",
    "        o = o.reshape(1,nclasses)\n",
    "        # From y we want to predict y[1:]. at y[t], predict y[t+1]\n",
    "        loss += F.cross_entropy(o, torch.tensor([y[t+1]], device=device))\n",
    "\n",
    "        p = softmax(o)\n",
    "        correct += torch.argmax(p[0]).item()==y[t+1]\n",
    "#         epoch_training_accur += correct\n",
    "    return loss, int(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss 19.0493 accur  0.5371   LR 0.001625\n",
      "Epoch   2 training loss 11.3777 accur  0.6930   LR 0.002750\n",
      "Epoch   3 training loss 10.3474 accur  0.6943   LR 0.003875\n",
      "Epoch   4 training loss 10.5952 accur  0.6432   LR 0.005000\n",
      "Epoch   5 training loss  9.2944 accur  0.6432   LR 0.003875\n",
      "Epoch   6 training loss  7.8974 accur  0.6278   LR 0.002750\n",
      "Epoch   7 training loss  6.6700 accur  0.6455   LR 0.001625\n",
      "Epoch   8 training loss  5.7310 accur  0.6273   LR 0.000500\n",
      "Epoch   9 training loss  5.1907 accur  0.5993   LR 0.001063\n",
      "Epoch  10 training loss  4.4192 accur  0.6648   LR 0.001625\n",
      "Epoch  11 training loss  4.6658 accur  0.6784   LR 0.002188\n",
      "Epoch  12 training loss  4.9271 accur  0.6851   LR 0.002750\n",
      "Epoch  13 training loss  5.0474 accur  0.6902   LR 0.002188\n",
      "Epoch  14 training loss  4.4758 accur  0.6930   LR 0.001625\n",
      "Epoch  15 training loss  4.4699 accur  0.6915   LR 0.001063\n",
      "Epoch  16 training loss  3.8225 accur  0.7038   LR 0.000500\n",
      "Epoch  17 training loss  3.2103 accur  0.7277   LR 0.000781\n",
      "Epoch  18 training loss  2.7562 accur  0.7547   LR 0.001063\n",
      "Epoch  19 training loss  2.7757 accur  0.7621   LR 0.001344\n",
      "Epoch  20 training loss  2.8239 accur  0.7544   LR 0.001625\n",
      "Epoch  21 training loss  3.1051 accur  0.7447   LR 0.001344\n",
      "Epoch  22 training loss  3.1362 accur  0.7395   LR 0.001063\n",
      "Epoch  23 training loss  2.7076 accur  0.7591   LR 0.000781\n",
      "Epoch  24 training loss  2.3953 accur  0.7778   LR 0.000500\n",
      "Epoch  25 training loss  2.0819 accur  0.8079   LR 0.000641\n",
      "Epoch  26 training loss  1.8339 accur  0.8248   LR 0.000781\n",
      "Epoch  27 training loss  1.7758 accur  0.8230   LR 0.000922\n",
      "Epoch  28 training loss  1.9565 accur  0.8135   LR 0.001063\n",
      "Epoch  29 training loss  2.1756 accur  0.7958   LR 0.000922\n",
      "Epoch  30 training loss  1.9981 accur  0.8117   LR 0.000781\n",
      "Epoch  31 training loss  1.8042 accur  0.8317   LR 0.000641\n",
      "Epoch  32 training loss  1.6059 accur  0.8464   LR 0.000500\n",
      "Epoch  33 training loss  1.4323 accur  0.8685   LR 0.000570\n",
      "Epoch  34 training loss  1.3484 accur  0.8788   LR 0.000641\n",
      "Epoch  35 training loss  1.3096 accur  0.8739   LR 0.000711\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "Ex = torch.randn(embed_sz,     len(X_vocab),  device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,       embed_sz,      device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "bx = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "by = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "bo = torch.zeros(nclasses,     1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "Ey = torch.randn(y_embed_sz,   len(Y_vocab),  device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W2 = torch.eye(nhidden,        nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "#C = torch.randn(nhidden,       nhidden,       device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "U2 = torch.randn(nhidden,      y_embed_sz,    device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "V = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([Ex,W,U,Ey,W2,U2,V,bx,by,bo], lr=0.001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.0005, max_lr=0.005,\n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "history = []\n",
    "epochs = 35\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total_compares = 0\n",
    "    for i in range(n):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        loss, correct = forward(x, y)\n",
    "        epoch_training_accur += correct\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "        total_compares += len(y) - 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_training_loss /= n\n",
    "    epoch_training_accur /= total_compares\n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x):\n",
    "    n = len(x)\n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "        # ENCODER\n",
    "        h = torch.zeros(nhidden, 1, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(len(x)):\n",
    "            embedding_step_t = Ex[:,x[t]]\n",
    "            embedding_step_t = embedding_step_t.reshape(embed_sz,1)\n",
    "            h = W @ h + U @ embedding_step_t + bx\n",
    "            h = torch.tanh(h)\n",
    "\n",
    "        # DECODER\n",
    "        y = [Y_vocab['<']] # begin with \"start of sequence\" char\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        while y!=Y_vocab['>']:\n",
    "            embedding_step_t = Ey[:,y]\n",
    "            embedding_step_t = embedding_step_t.reshape(y_embed_sz,1)\n",
    "            h = W2 @ h + U2 @ embedding_step_t + by\n",
    "            h = torch.tanh(h)\n",
    "            o = V @ h + bo\n",
    "            o = o.reshape(1,nclasses)\n",
    "            p = softmax(o[0])\n",
    "            y = torch.argmax(p).item()\n",
    "            if y!=Y_vocab['>']:\n",
    "                output.append(Y_idx[y])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 28, 10] ['one', 'hundred', 'ten']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred ten\".split()]\n",
    "print(x, [X_idx[n] for n in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one'] => ['1']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'hundred'] => ['1', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'hundred', 'ten'] => ['1', '1', '4']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred ten\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'hundred', 'thirty', 'two'] => ['1', '3', '2']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred thirty two\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eleven'] => ['1', '1']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"eleven\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ninety', 'nine'] => ['9', '9']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"ninety nine\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fifty', 'three'] => ['5', '5', '5']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"fifty three\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
