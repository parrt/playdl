{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate obama speeches using truncated back propagation and add embedding layer instead of one-hot encoding going into RNN\n",
    "\n",
    "Lessons:\n",
    "\n",
    "* w/o nonlinearity on embedding layer (I don't think people use nonlinearity for this). yep, adding embedding of chars before RNN helps. make len(vocab)->small embedding like 20 squeezes into more meaningful embedding than one of size len(vocab). After 20 epochs, was only 47% accurate before with:\n",
    "```\n",
    "lr = 0.001\n",
    "obama 100k text\n",
    "nchunks = 100\n",
    "nhidden = 512\n",
    "bptt = 8\n",
    "char embed size is 20\n",
    "```\n",
    "and is now 57% accurate! Got to 59% at 30 epochs.\n",
    "* Rather than one-hotting entire 2D input matrix, much smaller on GPU with embeddings.\n",
    "* With 1M char, 30 epochs same hyperparams gives 61% accur\n",
    "* Bumping to 2M char seems to help\n",
    "* With 1M char, making char embed size same as vocab len is converging slightly more slowly.  char embed size of 10 also less good (from 20). char embed size 30 seems about same.\n",
    "* Back to default args above. Increase nhidden to 600 from 512. seems much slower per epoch and not converging as fast. Trying 400: seems about same as 512.\n",
    "* Setting stddev to 0.01 for randn init seems to help. At epoch 6, (lr=0.001) we get 59% vs 56% accuracy (400 nhidden). 64% accurate at 30 epochs.\n",
    "* bptt from 8 to 16 is slower to converge but catches up.\n",
    "* nchunks 50 from 100 about same\n",
    "* nchunks 200 from 100 slower to converge even when bumping lr\n",
    "* 100 training records, chunk length 10000, vocab size 70, char_embed_sz 20, state is 400-vector; lr=0.001 dropping by .8 every 3 got me to 65% accurate.\n",
    "* with 100 epochs, got to 67% accurate with `lr_scheduler.StepLR(optimizer, step_size=10, gamma=.9)`:\n",
    "```\n",
    "...\n",
    "Epoch  99 training loss 10488.16   accur  0.6730   LR 0.000387\n",
    "Epoch 100 training loss 10490.52   accur  0.6727   LR 0.000349\n",
    "```\n",
    "* Same LR plan and with 2M text:\n",
    "```\n",
    "Epoch  99 training loss 21393.74   accur  0.6671   LR 0.000387\n",
    "Epoch 100 training loss 21388.15   accur  0.6674   LR 0.000349\n",
    "```\n",
    "vocab size seems to be increasing with increased text so should probably increase other hyperparameters\n",
    "* with all 4M text doesn't help so must need more complex model\n",
    "```\n",
    "Epoch  99 training loss 45675.70   accur  0.6647   LR 0.000387\n",
    "Epoch 100 training loss 45670.77   accur  0.6649   LR 0.000349\n",
    "```\n",
    "* Adding bias (matrix) seems to help a tiny bit (tanh activation), at least for first 10 epochs i watched.\n",
    "* Ah! Mystery solved. relu for RNN only works with bias term, otherwise get NaN immediately.\n",
    "* relu gets to 64% accur at 20 epochs. \n",
    "* Dang. relu still sometimes explodes. ah. must use F.cross_entropy() due to numerical instability.\n",
    "* Added V2 final layer after V. Worse at same parameters. Could require different learning rate.\n",
    "\n",
    "Stacked:\n",
    "\n",
    "* Must have low nchunks like 32. 100 didn't work. not stochastic enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, mode='r') as f:\n",
    "        s = f.read()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,          \n",
    "          mean=0.0, std=0.01, requires_grad=False,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float64):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend()#loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)\n",
    "\n",
    "def cross_entropy(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    y_pred is n x k for n samples and k output classes and y_true is n x 1\n",
    "    and is often softmax of final layer.\n",
    "    y_pred values must be probability that output is a specific class.\n",
    "    Binary case: When we have y_pred close to 1 and y_true is 1,\n",
    "    loss is -1*log(1)==0. If y_pred close to 0 and y_true is 1, loss is\n",
    "    -1*log(small value) = big value.\n",
    "    y_true values must be positive integers in [0,k-1].\n",
    "    \"\"\"\n",
    "    if torch.isnan(y_prob).any():\n",
    "        raise ValueError(\"cross_entropy: y_prob has NaN!\",y_prob)\n",
    "    n = y_prob.shape[0]\n",
    "    # Get value at y_true[j] for each sample with fancy indexing\n",
    "    p = y_prob[range(n),y_true]\n",
    "    p_ = p.detach()\n",
    "    if torch.isnan(p).any():\n",
    "        raise ValueError(\"cross_entropy: p has NaN! p=\",p_,\"y_prob=\",y_prob)\n",
    "    if (p_<0).any():\n",
    "        raise ValueError(\"cross_entropy: y_prob has negative value!:\",p_)\n",
    "    m = torch.mean(-torch.log(p))\n",
    "    if torch.isnan(m):\n",
    "        raise ValueError(\"cross_entropy: mean is NaN! p=\",p_)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split into chunks\n",
    "\n",
    "The stochastic part of SGD is critical for training models. The idea is simply to use a small subset of the data when computing gradients to update the model parameters. Generally we take a small batch size of say 32 records, run that through the model, and then compute a loss. From that loss we compute the gradient and then update the model parameters and move onto the next batch.  Once all batches are complete, we have completed an epoch.  We should shuffle the batches and keep going.\n",
    "\n",
    "We can also be stochastic by updating the gradient in the middle of long sequences, rather than waiting until after a complete batch of long sequences.  If the sequences are really long, waiting till the end of a batch reduces the stochastic nature. Instead I'm going to try breaking up the entire input into a small number of very long sequences. In this way the RNN can keep the hidden state going for the complete sequence. Of course the only problem is that we cannot compute back propagation that far, so at some sequence length I can update the gradient and wipe it out then continue. I think this is easier than modifying the data set stride so that a standard training loop for an RNN keeps the same hidden state across long sequences even if we have broken into chunks.\n",
    "\n",
    "Let's say that we have a large text and we break it up into six chunks: A,B,C,D,E,F. then, six is our batch size and we will process each long sequence exactly once per epic. However to get stochastic nature, we will update the gradient after only a small sequence of characters.  We pick the chunk size and then the batch sizes computed instead of having to specify both. I think the chunk size is more important: how much can you store in a single hidden state vector.\n",
    "\n",
    "Come to think of it, all we need to specify is the number of chunks we want to break the text into.  There won't be any batch size because we have a single batch with `nchunks`  long records in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4224143"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_text(\"data/obama-speeches.txt\").lower() # generated from obama-sentences.py\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[0:200_000] # testing\n",
    "n = len(text)\n",
    "\n",
    "bptt = 16                 # only look back this many time steps for gradients\n",
    "nhidden = 200\n",
    "char_embed_sz = 20        # there are 50+ chars, squeeze down into fewer dimensions for embedding prior to input into RNN \n",
    "nchunks = 32              # break up the input into a number of chunks (doesn't have to be small like batch size)\n",
    "chunk_size = n // nchunks # the sequences will be very long\n",
    "n = nchunks * chunk_size  # reset size so it's an even multiple of chunk size\n",
    "text = text[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [text[p:p+chunk_size] for p in range(0, n, chunk_size)]\n",
    "X = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long) # int8 doesn't work as indices\n",
    "y = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long)\n",
    "for i,chunk in enumerate(chunks):\n",
    "    X[i,:] = torch.tensor([ctoi[c] for c in chunk[0:-1]], device=device)\n",
    "    y[i,:] = torch.tensor([ctoi[c] for c in chunk[1:]],   device=device)\n",
    "    \n",
    "# X, y are now chunked and numericalized into big 2D matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 training records, chunk length 6250, vocab size 55, char_embed_sz 20, state is 200-vector\n"
     ]
    }
   ],
   "source": [
    "nclasses = len(ctoi)\n",
    "print(f\"{nchunks:,d} training records, chunk length {chunk_size}, vocab size {len(ctoi)}, char_embed_sz {char_embed_sz}, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 6249]), 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, nchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "E = torch.randn(char_embed_sz, len(ctoi),     device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,       char_embed_sz, device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "bx = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "by = torch.zeros(nclasses,     1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "V = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "# if using relu, b must be 0. W must be identity so don't mess with sd. others must have low stdev\n",
    "# From [Le 2015] https://arxiv.org/abs/1504.00941\n",
    "# \"For IRNNs, in addition to the recurrent weights being initialized at identity, the non-recurrent\n",
    "#  weights are initialized with a random matrix, whose entries are sampled from a\n",
    "#  Gaussian distribution with mean of zero and standard deviation of 0.001.\"\n",
    "sd = 0.001  # weight stddev init for relu\n",
    "sd = 0.01   # weight stddev init for tanh\n",
    "with torch.no_grad():\n",
    "    E *= sd\n",
    "    U *= sd\n",
    "    V *= sd\n",
    "    \n",
    "# gradient clipping values \n",
    "gc = {1, 10, 100, 1000}\n",
    "\n",
    "parameters = [E,W,U,bx,by,V]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.0005, weight_decay=0.0)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.0001, max_lr=0.0008,\n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "#     shuffled_idx = torch.randperm(nchunks) # shuffle each epoch (don't need actually)\n",
    "    H = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    loss = 0\n",
    "    for t in range(chunk_size-1):  # char t in chunk predicts t+1 so one less\n",
    "#         print(f\"t={t}\")\n",
    "        chars_step_t = X[:,t] # char_embed_sz x nchunks\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        embedding_step_t = E[:,chars_step_t] # char_embed_sz x nchunks\n",
    "#         print(embedding_step_t.shape, E.shape, H.shape, W.shape, U.shape)\n",
    "        H = W.mm(H) + U.mm(embedding_step_t) + bx\n",
    "        H = torch.tanh(H)\n",
    "#         H = torch.relu(H)\n",
    "        o = V.mm(H) + by\n",
    "        o = o.T # make it nchunks x nclasses\n",
    "        p = softmax(o)\n",
    "        correct = torch.argmax(p, dim=1)==y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "#         print(f\"loss {loss:7.4f}\")\n",
    "#         loss += cross_entropy(p, y[:,t])\n",
    "        loss += F.cross_entropy(o, y[:,t])\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "#             print(f\"gradient at {t:4d}, loss {loss.item():7.4f}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "#             torch.nn.utils.clip_grad_value_(parameters, 10)  # gradient clipping when using relu\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            H = H.detach() # no longer consider previous computations\n",
    "\n",
    "    epoch_training_accur /=  nchunks * (chunk_size-1)\n",
    "    epoch_training_loss /= bptt * nchunks\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.2f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve sampler to avoid recomputing whole string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_sample(initial_chars, n, temperature=0.1):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    chars = initial_chars\n",
    "    n -= len(initial_chars)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            h = torch.zeros(nhidden, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "            for j in range(len(chars)):  # for each char in a name\n",
    "                c = chars[j]\n",
    "                ci = ctoi[c]\n",
    "                embedding_step_j = E[:,ci].reshape(char_embed_sz,1) # col is embedding for c; must be column\n",
    "#                 print(embedding_step_j.shape, E.shape, h.shape, W.shape, U.shape)#, V.shape)\n",
    "                h = W@h + U@embedding_step_j + bx\n",
    "                h = torch.tanh(h)\n",
    "#                 h = torch.relu(h)\n",
    "            o = V@h + by\n",
    "            o = o.reshape(nclasses)\n",
    "            p = softmax(o)\n",
    "#             wi = torch.argmax(p) # this doesn't work (just repeats 'and' a million times)\n",
    "            wi = np.random.choice(range(len(vocab)), p=p.cpu()) # don't always pick most likely; pick per distribution\n",
    "            chars.append(vocab[wi])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n):\n",
    "    n -= len(initial_chars)\n",
    "    output = initial_chars.copy()\n",
    "    with torch.no_grad():\n",
    "        # get h for initial char\n",
    "        h = torch.zeros(nhidden, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(len(initial_chars)-1):\n",
    "            c = initial_chars[t]\n",
    "            ci = ctoi[c]\n",
    "            embedding_step_t = E[:,ci].reshape(char_embed_sz,1)\n",
    "            h = W@h + U@embedding_step_t + bx\n",
    "            h = torch.tanh(h)\n",
    "\n",
    "        ci = ctoi[initial_chars[-1]] # get last initial char (it's unprocessed)\n",
    "        \n",
    "        for i in range(n):\n",
    "            embedding_step_t = E[:,ci].reshape(char_embed_sz,1) # col is embedding for c; must be column\n",
    "            h = W@h + U@embedding_step_t + bx\n",
    "            h = torch.tanh(h)\n",
    "            o = V@h\n",
    "            o = o.reshape(nclasses)\n",
    "            p = softmax(o)\n",
    "            ci = np.random.choice(range(len(vocab)), p=p.cpu()) # don't always pick most likely; pick per distribution\n",
    "            output.append(vocab[ci])\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor RNN into object or function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, input_size, embed_sz):\n",
    "        self.E = torch.randn(embed_sz, input_size, device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "        self.input_size = input_size\n",
    "        self.embed_sz = embed_sz\n",
    "        with torch.no_grad():\n",
    "            self.E *= 0.01\n",
    "    def __call__(self, x):\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        return self.E[:,x].reshape(self.embed_sz,len(x)) # embed_sz by len(x) (is this shape except when len(x)==1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_sz, nhidden):\n",
    "        self.W = torch.eye(nhidden,    nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.U = torch.randn(nhidden,  input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.bx = torch.zeros(nhidden, 1,        device=device, dtype=torch.float64, requires_grad=True)\n",
    "        with torch.no_grad():\n",
    "            self.W *= 0.01\n",
    "            self.U *= 0.01\n",
    "    def __call__(self, h, x):\n",
    "        h = self.W@h + self.U@x + self.bx\n",
    "        h = torch.tanh(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.V = torch.randn(output_size,  input_size, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.by = torch.zeros(output_size, 1,          device=device, dtype=torch.float64, requires_grad=True)\n",
    "        with torch.no_grad():\n",
    "            self.V *= 0.01\n",
    "    def __call__(self, h):\n",
    "        o = self.V@h + self.by\n",
    "        o = o.T # make it input_size x output_size\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss    45.25   accur  0.1724   LR 0.000258\n",
      "Epoch   2 training loss    36.15   accur  0.1755   LR 0.000505\n",
      "Epoch   3 training loss    36.08   accur  0.1755   LR 0.000753\n",
      "Epoch   4 training loss    33.61   accur  0.2367   LR 0.001000\n",
      "Epoch   5 training loss    29.55   accur  0.2962   LR 0.000753\n",
      "Epoch   6 training loss    27.50   accur  0.3364   LR 0.000505\n",
      "Epoch   7 training loss    26.28   accur  0.3682   LR 0.000258\n",
      "Epoch   8 training loss    25.56   accur  0.3841   LR 0.000010\n",
      "Epoch   9 training loss    25.28   accur  0.3896   LR 0.000134\n",
      "Epoch  10 training loss    25.18   accur  0.3921   LR 0.000258\n",
      "Epoch  11 training loss    24.90   accur  0.3978   LR 0.000381\n",
      "Epoch  12 training loss    24.48   accur  0.4072   LR 0.000505\n",
      "Epoch  13 training loss    23.92   accur  0.4192   LR 0.000381\n",
      "Epoch  14 training loss    23.34   accur  0.4324   LR 0.000258\n",
      "Epoch  15 training loss    22.93   accur  0.4420   LR 0.000134\n",
      "Epoch  16 training loss    22.68   accur  0.4474   LR 0.000010\n",
      "Epoch  17 training loss    22.56   accur  0.4498   LR 0.000072\n",
      "Epoch  18 training loss    22.53   accur  0.4505   LR 0.000134\n",
      "Epoch  19 training loss    22.45   accur  0.4524   LR 0.000196\n",
      "Epoch  20 training loss    22.31   accur  0.4554   LR 0.000258\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding(len(ctoi), char_embed_sz)\n",
    "rnn = RNN(char_embed_sz, nhidden)\n",
    "lin = Linear(nhidden, nclasses)\n",
    "parameters = [emb.E,\n",
    "              rnn.W,rnn.U,rnn.bx,\n",
    "              lin.V,lin.by]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.0005, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "#                                               base_lr=0.0001, max_lr=0.0008,\n",
    "                                              base_lr=0.00001, max_lr=0.001,\n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    H = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    loss = 0\n",
    "    for t in range(chunk_size-1):  # char t in chunk predicts t+1 so one less\n",
    "        x = emb(X[:,t]) # char_embed_sz x nchunks\n",
    "        H = rnn(H, x)\n",
    "        o = lin(H)\n",
    "\n",
    "        loss += F.cross_entropy(o, y[:,t])\n",
    "\n",
    "        p = softmax(o)        \n",
    "        correct = torch.argmax(p, dim=1)==y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            H = H.detach() # no longer consider previous computations\n",
    "\n",
    "    epoch_training_accur /=  nchunks * (chunk_size-1)\n",
    "    epoch_training_loss /= bptt * nchunks\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.2f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus(probabilities, P=.95):\n",
    "    \"\"\"\n",
    "    Given probabilities array (summing to 1.0), find and return new array\n",
    "    containing just the largest probabilities that sum to less than or\n",
    "    equal to P. Normalize to sum to 1.0 by dividing by new sum. All\n",
    "    other probabilities are 0.\n",
    "    \"\"\"\n",
    "    P = max(P,torch.max(probabilities))\n",
    "    p_idx = torch.flip( torch.argsort(probabilities), dims=[0] )\n",
    "    c = torch.cumsum(probabilities[p_idx], dim=0)\n",
    "    probabilities_ = torch.zeros_like(probabilities)\n",
    "    top_P = p_idx[torch.where(c<=P)[0]]\n",
    "    probabilities_[top_P] = probabilities[top_P]\n",
    "    return probabilities_ / torch.sum(probabilities_) # normalize so sum is 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, use_nucleus=True):    \n",
    "    n -= len(initial_chars)\n",
    "    output = initial_chars.copy()\n",
    "    with torch.no_grad():\n",
    "        # get h for initial char\n",
    "        h = torch.zeros(nhidden, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(len(initial_chars)-1):\n",
    "            ci = ctoi[initial_chars[t]]\n",
    "            x = emb(torch.tensor([ci])) # char_embed_sz x nchunks\n",
    "            h = rnn(h, x)\n",
    "\n",
    "        ci = ctoi[initial_chars[-1]] # get last initial char (it's unprocessed)\n",
    "        \n",
    "        for i in range(n):\n",
    "            x = emb(torch.tensor([ci]))\n",
    "            h = rnn(h, x)\n",
    "            o = lin(h)\n",
    "            p = softmax(o).flatten()\n",
    "            if use_nucleus:\n",
    "                p = nucleus(p)\n",
    "            ci = np.random.choice(range(len(vocab)), p=p.cpu()) # don't always pick most likely; pick per distribution\n",
    "            output.append(vocab[ci])\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes we can we died in paisers.  and prald us a ferithing bis i on diving the goes a similies becaas lenging that bo dedper a sigktion as the aclessen, but araqt a corlaptods and gins in to be then your bore a coulle to mo the sterity lester fare mitien and innoal hear ples and ofe meagt of the pant '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join( sample(list('yes we can'), 300) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try stacked RNN\n",
    "\n",
    "This doesn't seem to train, likely because it's now too deep to pass the gradients all the way back.\n",
    " \n",
    "Do shortcircuit of x. add x to each column of H. Nope. doesn't work either.\n",
    "\n",
    "Ah! Must have low nchunks like 32. 100 didn't work. not stochastic enough.\n",
    "\n",
    "So, stacking works to improve things a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss   44.023   accur  0.1725   LR 0.000258\n",
      "Epoch   2 training loss   36.139   accur  0.1755   LR 0.000505\n",
      "Epoch   3 training loss   36.112   accur  0.1755   LR 0.000753\n",
      "Epoch   4 training loss   36.133   accur  0.1755   LR 0.001000\n",
      "Epoch   5 training loss   33.650   accur  0.2233   LR 0.000753\n",
      "Epoch   6 training loss   28.166   accur  0.3246   LR 0.000505\n",
      "Epoch   7 training loss   26.171   accur  0.3689   LR 0.000258\n",
      "Epoch   8 training loss   25.286   accur  0.3901   LR 0.000010\n",
      "Epoch   9 training loss   24.967   accur  0.3967   LR 0.000134\n",
      "Epoch  10 training loss   24.856   accur  0.4006   LR 0.000258\n",
      "Epoch  11 training loss   24.564   accur  0.4089   LR 0.000381\n",
      "Epoch  12 training loss   24.119   accur  0.4196   LR 0.000505\n",
      "Epoch  13 training loss   23.588   accur  0.4310   LR 0.000381\n",
      "Epoch  14 training loss   23.033   accur  0.4430   LR 0.000258\n",
      "Epoch  15 training loss   22.642   accur  0.4502   LR 0.000134\n",
      "Epoch  16 training loss   22.389   accur  0.4554   LR 0.000010\n",
      "Epoch  17 training loss   22.262   accur  0.4583   LR 0.000072\n",
      "Epoch  18 training loss   22.239   accur  0.4588   LR 0.000134\n",
      "Epoch  19 training loss   22.167   accur  0.4607   LR 0.000196\n",
      "Epoch  20 training loss   22.041   accur  0.4637   LR 0.000258\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding(len(ctoi), char_embed_sz)\n",
    "rnn = RNN(char_embed_sz, nhidden)\n",
    "rnn2 = RNN(nhidden+char_embed_sz, nhidden)  # this RNN combines x and hidden as input\n",
    "lin = Linear(nhidden, nclasses)\n",
    "parameters = [emb.E,\n",
    "              rnn.W,rnn.U,rnn.bx,\n",
    "              rnn2.W,rnn2.U,rnn2.bx,\n",
    "              lin.V,lin.by]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.00001, max_lr=0.001,\n",
    "                                              cycle_momentum=False)\n",
    "history = []\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    H = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    H2 = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    loss = 0\n",
    "    for t in range(chunk_size-1):  # char t in chunk predicts t+1 so one less\n",
    "        x = emb(X[:,t]) # char_embed_sz x nchunks\n",
    "        H = rnn(H, x)\n",
    "#         print(H.shape, H2.shape, x.shape, torch.cat([H,x]).shape)\n",
    "        H2 = rnn2(H2, torch.cat([H,x]))\n",
    "        o = lin(H2)\n",
    "\n",
    "        loss += F.cross_entropy(o, y[:,t])\n",
    "\n",
    "        p = softmax(o)        \n",
    "        correct = torch.argmax(p, dim=1)==y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            H = H.detach() # no longer consider previous computations\n",
    "            H2 = H2.detach() # no longer consider previous computations\n",
    "\n",
    "    epoch_training_accur /=  nchunks * (chunk_size-1)\n",
    "    epoch_training_loss /= bptt * nchunks\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.3f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
