{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text tokenization, word indexing, word vectors\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/parrt/playdl/blob/master/mnist/notebooks/word-vectors.ipynb)\n",
    "\n",
    "Given a document represented as a string of characters, we need to break it into words or tokens, which is sometimes harder than you think given all of the weird punctuation stuff. But, once we get individual tokens, we need to represent them numerically, rather than as strings. The easiest approach is simply to assign each word in the vocabulary a unique number, which corresponds to label encoding of categorical variables. If we have a model that accepts a sequence of words, we can just pass it a sequence of vocabulary index numbers.\n",
    "\n",
    "The problem is that linear models don't handle labeling coding very well, preferring dummy and coded variables instead. So, for use with neural networks, we need to one-hot encode token index values. These are very sparse vectors. Of course those can get quite large and so we can use a so-called hashing trick to constrain the size of the vector used to represent each work. That also has the advantage that we don't need to hold a dictionary of Word to index value in memory. Better yet will be to represent words as dense vectors, which I will look at in [word-embeddings.ipynb](word-embeddings.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder why we need to one hot encode categorical variables\n",
    "\n",
    "Notice that a random forest does a great job with labeling coded categorical variables but a linear model is terrible. 0.974 R^2 for RF vs 0.035 for the linear model. BUT, if we one hot encode then the linear model gets a perfect training score. (I don't care about generality here, just how well a model is able to capture relationships.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest on simple car model identifier to price regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF training R^2 score 1.000\n",
      "RF valid R^2 score 0.125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "CarModel = np.array([3807, 4140, 7026, 260, 4991, 11398, 18174, 1130, 12924, 4333, 14304, 4605, 18473, 2137, 9551, 9550, 4657, 5633, 17976, 22155, 3180, 2160, 5527, 19340, 22162, 23926, 3861, 3856, 4605, 18588, 23752, 3178, 10497, 3409, 1355, 6793, 77, 3362, 5888])\n",
    "CarModel = CarModel.reshape(-1,1)\n",
    "Price = np.array([47500, 60000, 10000, 65000, 12500, 50000, 41000, 59000, 17500, 25000, 37000, 24500, 7000, 34000, 9500, 9500, 9000, 10500, 21500, 50000, 27500, 27000, 11000, 19750, 46000, 114000, 62500, 67500, 28000, 10500, 85000, 16500, 23000, 10500, 37000, 11000, 39000, 58000, 41000])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(CarModel, Price, test_size=0.2)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=30)\n",
    "rf.fit(X_train, y_train)\n",
    "train_score = rf.score(X_train, y_train)\n",
    "print(f\"RF training R^2 score {train_score:.3f}\")\n",
    "valid_score = rf.score(X_test, y_test)\n",
    "print(f\"RF valid R^2 score {valid_score:.3f}\") # it's crap but not the point here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model on simple car model identifier to price regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model training R^2 score 0.084\n",
      "Linear model valid R^2 score -0.900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "train_score = lm.score(X_train, y_train)\n",
    "print(f\"Linear model training R^2 score {train_score:.3f}\")\n",
    "valid_score = lm.score(X_test, y_test)\n",
    "print(f\"Linear model valid R^2 score {valid_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, now, let's one hot encode the model identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarModel now has 38 columns\n",
      "Linear model training R^2 score 1.000\n",
      "Linear model valid R^2 score -0.065\n"
     ]
    }
   ],
   "source": [
    "CarModel = np.array([3807, 4140, 7026, 260, 4991, 11398, 18174, 1130, 12924, 4333, 14304, 4605, 18473, 2137, 9551, 9550, 4657, 5633, 17976, 22155, 3180, 2160, 5527, 19340, 22162, 23926, 3861, 3856, 4605, 18588, 23752, 3178, 10497, 3409, 1355, 6793, 77, 3362, 5888])\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['CarModel'] = CarModel.reshape(-1)\n",
    "df['CarModel'] = df['CarModel'].astype('category')\n",
    "CarModel = pd.get_dummies(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(CarModel, Price, test_size=0.2)\n",
    "\n",
    "print(f\"CarModel now has {CarModel.shape[1]} columns\")\n",
    "lm.fit(X_train, y_train)\n",
    "train_score = lm.score(X_train, y_train)\n",
    "print(f\"Linear model training R^2 score {train_score:.3f}\")\n",
    "valid_score = lm.score(X_test, y_test)\n",
    "print(f\"Linear model valid R^2 score {valid_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I don't care about generality here, just how well a model is able to capture relationships.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using keras to tokenize and numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample tweets from my twitter inbox with added text for experimentation\n",
    "samples = [\n",
    "    \"\"\"Tesla Motors has nothing to do with this tweet.\n",
    "    On those rare occasions when I really, really need to reduce the\n",
    "    size of a file I use \"xz -9\". Today I found out about the \"extreme\" setting\n",
    "    and \"xz -e9\" squeezed files down another 15% or so. It is not exactly quick,\n",
    "    but that doesn't really matter in such cases!\"\"\",\n",
    "    \n",
    "    \"\"\"Securities and exchange commission has nothing to do with this tweet.\n",
    "    Do grad students get paid a lot? No. But do we at least have solid\n",
    "    job security? Also, no. But are we at least ensured a stress-free work\n",
    "    environment with a healthy work-life balance? Still, also no.\"\"\",\n",
    "\n",
    "    \"\"\"A design process hyperfocused on A/B testing can result in dark patterns even\n",
    "    if that’s not the intent. That’s because most A/B tests are based on metrics\n",
    "    that are relevant to the company’s bottom line, even if they result in harm to users.\"\"\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=30)\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 14,  2,  3,  5, 15, 16,  6,  7,  8,  8,  2,  4,  1,  7, 17,  7,\n",
       "        4, 18, 17, 19,  9, 20,  8, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "np.array(sequences[0]) # token sequence for first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot for samples\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 1., 2., 1., 1., 3., 3., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 2., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 3., 1., 3., 0., 2., 0., 0., 0., 2., 0., 3., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 2., 2., 2., 2., 2., 0., 0., 0., 0.],\n",
       "       [0., 3., 2., 0., 2., 0., 2., 0., 0., 0., 2., 0., 2., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count in one-hot positions for samples\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='count')\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in these examples that we are able to encode an entire document with a single vector because we can just turn on the appropriate word positions if that word is present in the document. This works because of the sparsity of the vector. If we had dense factors, we might have to add the vectors together and get a center of mass to represent a document or we need a sequence of word vectors to represent a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"One-hot hash trick\"\n",
    "\n",
    "Instead of using unique word index into a dictionary of unique words from all samples, compute hash function on word as its code. Lets us limit size of word vector to fixed length rather than length of vocab or whatever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX: {'a': 7, 'to': 9, 'do': 1, 'the': 7, 'with': 0, 'on': 8, 'i': 5, 'really': 9, 'but': 4, 'in': 0, 'no': 1, 'are': 1, 'has': 7, 'nothing': 5, 'this': 3, 'tweet': 2, 'xz': 2, 'and': 8, 'not': 4, 'that': 0, 'we': 3, 'at': 2, 'least': 4, 'also': 7, 'work': 1, 'b': 8, 'result': 6, 'even': 2, 'if': 2, 'that’s': 1, 'tesla': 9, 'motors': 1, 'those': 9, 'rare': 9, 'occasions': 5, 'when': 2, 'need': 2, 'reduce': 9, 'size': 7, 'of': 0, 'file': 9, 'use': 9, '9': 7, 'today': 5, 'found': 2, 'out': 6, 'about': 4, 'extreme': 5, 'setting': 9, 'e9': 5, 'squeezed': 2, 'files': 7, 'down': 6, 'another': 4, '15': 5, 'or': 2, 'so': 1, 'it': 6, 'is': 5, 'exactly': 3, 'quick': 1, \"doesn't\": 2, 'matter': 2, 'such': 4, 'cases': 1, 'securities': 9, 'exchange': 7, 'commission': 0, 'grad': 8, 'students': 5, 'get': 6, 'paid': 2, 'lot': 6, 'have': 1, 'solid': 4, 'job': 0, 'security': 5, 'ensured': 2, 'stress': 3, 'free': 9, 'environment': 4, 'healthy': 9, 'life': 3, 'balance': 3, 'still': 4, 'design': 0, 'process': 3, 'hyperfocused': 4, 'testing': 7, 'can': 2, 'dark': 7, 'patterns': 9, 'intent': 2, 'because': 7, 'most': 8, 'tests': 1, 'based': 0, 'metrics': 5, 'relevant': 2, 'company’s': 5, 'bottom': 3, 'line': 7, 'they': 7, 'harm': 7, 'users': 3}\n",
      "10/105=9.5% collisions using 10 unique codes\n"
     ]
    }
   ],
   "source": [
    "def hash(w):\n",
    "    h = 0\n",
    "    for c in w:\n",
    "        h = (h<<3) + ord(c)\n",
    "    return h\n",
    "\n",
    "def hashwords(words, dimensionality = 1000):\n",
    "    collisions = set()\n",
    "    codes = set()\n",
    "    index = {}\n",
    "    for w in words:\n",
    "        wcode = hash(w)%dimensionality\n",
    "        if wcode in codes:\n",
    "            collisions.add(wcode)\n",
    "        codes.add(wcode)\n",
    "        index[w] = wcode\n",
    "    return index, collisions\n",
    "\n",
    "words = tokenizer.word_index.keys() # get tokenized words\n",
    "dimensionality=10\n",
    "index, collisions = hashwords(words, dimensionality)\n",
    "print(\"INDEX:\", index)\n",
    "print(f\"{len(collisions)}/{len(words)}={(len(collisions)*100)/len(words):.1f}% collisions using {dimensionality} unique codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/105 =  9.5% collisions using     10 unique codes\n",
      "30/105 = 28.6% collisions using     50 unique codes\n",
      "29/105 = 27.6% collisions using    100 unique codes\n",
      " 7/105 =  6.7% collisions using    500 unique codes\n",
      " 4/105 =  3.8% collisions using   1000 unique codes\n",
      " 3/105 =  2.9% collisions using   2000 unique codes\n",
      " 1/105 =  1.0% collisions using   5000 unique codes\n",
      " 0/105 =  0.0% collisions using  10000 unique codes\n"
     ]
    }
   ],
   "source": [
    "for dimensionality in [10,50,100,500,1000,2000,5000,10000]:\n",
    "    index, collisions = hashwords(words, dimensionality)\n",
    "    print(f\"{len(collisions):2d}/{len(words):2d} = {(len(collisions)*100)/len(words):4.1f}% collisions using {dimensionality:6d} unique codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Using my typical hash function it takes a big dimensionality before we reduce collisions. In this case, there are only 96 unique words which means unique (perfect) hashing requires a dimensionality of only 96. The hashing trick requires thousands of hash buckets before we get 0 collisions. I suppose that if a simple hash function worked well, we would need wording embeddings. :)\n",
    "\n",
    "The advantage of this is that we don't need a dictionary in memory and it deals well with unknown words (words not available when we created the original dictionary/index). If we have a dictionary, unknown words will cause key not found errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spacy to tokenize, add meta data\n",
    "\n",
    "Before, assigning (hopefully unique) integers to each word,It's a good idea to processed the text more heavily and to insert special tags to represent information about the text.  Humans innately see a stream of words and construct a parse tree in their head, such as this is the subject and this is the verb. In order to help a model understand the text, giving hints about proper nouns, verbs, and sentence structure could be very useful.\n",
    "\n",
    "Spacy is great library for getting information about text. Check out [Spacy 101](https://spacy.io/usage/spacy-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "#!python -m spacy download en_core_web_sm  # requires restart of jupyter kernal afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def compress_whitespace(s): # collapse things like \"\\n   \\t  \" with \" \"\n",
    "    return re.sub(r\"(\\s+)\", ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tesla, Motors, has, nothing, to, do, with, this, tweet, ., On, those, rare, occasions, when, I, really, ,, really, need]\n",
      "[Securities, and, exchange, commission, has, nothing, to, do, with, this, tweet, ., Do, grad, students, get, paid, a, lot, ?]\n",
      "[A, design, process, hyperfocused, on, A, /, B, testing, can, result, in, dark, patterns, even, if, that, ’s, not, the]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # When I use plain English() it doesn't seem to give POS info\n",
    "for sample in samples:    \n",
    "    doc = nlp(compress_whitespace(sample))\n",
    "    tokens = list(doc)\n",
    "    print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add parts-of-speech meta data\n",
    "\n",
    "Adding some tricks from [Jeremy / Sylvain's awesome book](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) that indicate \"start of stream\" (`xxbos`) etc...  Use Spacy to add proper noun and verb tags.  Lowercase after Spacy has sniffed the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxbos xxpropn tesla xxpropn xxsubj motors has nothing to do with this tweet . on those rare occasions when xxsubj i really , really xxverb need to xxverb reduce the size of a file xxsubj i xxverb use \" xxpropn xz xxpropn -9 \" . today xxsubj i xxverb found\n",
      "xxbos xxpropn securities and xxpropn exchange xxpropn xxsubj commission has nothing to do with this tweet . do grad students get xxverb paid a lot ? no . but do xxsubj we at least have solid job security ? also , no . but are xxsubj we at least xxverb\n",
      "xxbos a design xxsubj process xxverb hyperfocused on xxpropn a / b testing xxverb can xxverb result in dark patterns even if xxsubj that xxverb ’s not the intent . that xxverb ’s because most a / b tests are xxverb based on metrics xxsubj that are relevant to the\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for sample in samples:\n",
    "    doc = nlp(compress_whitespace(sample))\n",
    "    tokens = ['xxbos']\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            tokens.append('xxverb')\n",
    "        if token.pos_=='PROPN':\n",
    "            tokens.append('xxpropn')\n",
    "        if token.dep_=='nsubj':           # is token subject of a sentence or phrase?\n",
    "            tokens.append('xxsubj')\n",
    "        tokens.append(str(token).lower())\n",
    "    tokens.append('xxeos')\n",
    "    print(' '.join(tokens[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify sentences and phrase subject\n",
    "\n",
    "Add `xxbegin` tokens before start of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxbos xxpropn xxbegin tesla xxpropn xxsubj motors has nothing to do with this tweet . xxbegin on those rare occasions when xxsubj i really , really xxverb need to xxverb reduce the size of a file xxsubj i xxverb use \" xxpropn xz xxpropn -9 \" . xxbegin today xxsubj\n",
      "xxbos xxpropn xxbegin securities and xxpropn exchange xxpropn xxsubj commission has nothing to do with this tweet . xxbegin do grad students get xxverb paid a lot ? xxbegin no . xxbegin but do xxsubj we at least have solid job security ? xxbegin also , no . xxbegin but\n",
      "xxbos xxbegin a design xxsubj process xxverb hyperfocused on xxpropn a / b testing xxverb can xxverb result in dark patterns even if xxsubj that xxverb ’s not the intent . xxbegin that xxverb ’s because most a / b tests are xxverb based on metrics xxsubj that are relevant\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import Sentencizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(Sentencizer())\n",
    "for sample in samples:\n",
    "    doc = nlp(compress_whitespace(sample))\n",
    "    tokens = ['xxbos']\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            tokens.append('xxverb')\n",
    "        if token.pos_=='PROPN':\n",
    "            tokens.append('xxpropn')\n",
    "        if token.dep_=='nsubj':           # is token subject of a sentence or phrase?\n",
    "            tokens.append('xxsubj')\n",
    "        if token.is_sent_start:           # beginning of sentence?\n",
    "            tokens.append('xxbegin')\n",
    "        tokens.append(str(token).lower())\n",
    "    tokens.append('xxeos')\n",
    "    print(' '.join(tokens[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have everything tokenized properly, we can then assign a unique number to the words and meta-tags."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
